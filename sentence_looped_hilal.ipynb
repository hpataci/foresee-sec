{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "FVEjMjoFOco9",
    "outputId": "eaeba079-050f-4df6-99cb-f2fe0b3b4ae4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hilal.pataci/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hilal.pataci/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk.data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "import types\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dJRQFHyKOcpB"
   },
   "outputs": [],
   "source": [
    "#txt='../sec_cleaning/txt/' #path of txt files from SEC-cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAlPoTIEOcpE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import glob,os\n",
    "from pathlib import Path\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "\n",
    "#print(os.getcwd())\n",
    "\n",
    "#os.chdir(\"../sec_cleaning/txt\")\n",
    "#print(os.getcwd())\n",
    "#breakpoint()\n",
    "rid = 0\n",
    "sentences = {}\n",
    "filenames=[]\n",
    "#read file path\n",
    "\n",
    "prefixed_files = [filename for filename in glob.glob(\"./txt/*.txt\") if filename.startswith(\"./txt/AAL\")]\n",
    "#print(prefixed_files)\n",
    "#for filename in glob.glob(\"./txt/*.txt\"):\n",
    "#    print(filename)\n",
    "for text_path in prefixed_files:\n",
    "    #print(\"foo\")\n",
    "    #read all txt file \n",
    "    text = Path(text_path).read_text()\n",
    "    filenames.append(os.path.basename(text_path))\n",
    "    #breakpoint()\n",
    "    #print(text)\n",
    "    #breakpoint()\n",
    "    #create a dictionary and assign each sentence to a dictionary.\n",
    "    #dictionary will hold the sentences of each file\n",
    "    sentences[rid] = []\n",
    "    text = text.strip()\n",
    "    #extract each sentence and assign to corresponding the dictionary\n",
    "    sentences[rid].extend(sent_detector.tokenize(text))\n",
    "    rid+=1\n",
    "\n",
    "print(len(sentences))\n",
    "#print(sentences[0])\n",
    "#print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "05S9d78eOcpG"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "causal_verbs = [\n",
    "    \"cause\", \"causes\",\"caused\",\n",
    "    \"result in\", \"results in\", \"resulted in\", \n",
    "    \"induce\", \"induces\", \"induced\", \n",
    "    \"give rise\", \"gives rise\", \"gave rise\", \"given rise\", \n",
    "    \"generate\", \"generates\", \"generated\", \n",
    "    \"bring about\", \"brings about\", \"brought about\", \n",
    "    \"lead to\", \"leads to\", \"led to\", \n",
    "    \"trigger\", \"triggers\", \"triggered\", \n",
    "    \"is linked to\", \"are linked to\", \"were linked to\", \"was linked to\", \n",
    "    \"brings forth\", \n",
    "    \"leads up to\", \"lead up to\", \"led up to\",\n",
    "    \"triggers off\", \"trigger off\", \"triggered off\", \n",
    "    \"bring on\", \"brings on\", \"brought on\",\n",
    "    \"affect\", \"affects\", \"affected\",\n",
    "    \"effect\", \"effects\", \"effected\",\n",
    "    \"instigate\", \"instigates\", \"instigated\",\n",
    "    \"stimulate\", \"stimulated\", \"stimulates\",\n",
    "    \"engender\", \"engendered\",\"engenders\", \n",
    "    \"impact\", \"impacts\", \"impacted\",\n",
    "    \"become ground\", \"became ground\", \"become grounds\", \"became grounds\"\n",
    "]\n",
    "reverse_causal_verbs = [\n",
    "    \"stems from\", \"stemmed from\", \"stem from\",\n",
    "    \"brought on by\", \n",
    "    \"caused by\", \n",
    "    \"affected by\", \"effected by\",\n",
    "    \"resulted from\", \"results from\", \"as a result of\",\n",
    "    \"emerged from\", \"emerges from\", \"carried out by\", \"invoked by\", \"occurred because\", \"in consequence\"\n",
    "]\n",
    "causal_re = re.compile(\" | \".join(causal_verbs), re.IGNORECASE)\n",
    "reverse_causal_re = re.compile(\" | \".join(reverse_causal_verbs), re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ug0BoZq6OcpI"
   },
   "outputs": [],
   "source": [
    "use_chunks = False\n",
    "chunking_method = \"nltk\"\n",
    "min_chunk_strlen = 6\n",
    "\n",
    "# The following is a temporary simple definition of get_ce\n",
    "def get_ce(part1, part2, use_chunks=False, chunking_method=\"nltk\", min_chunk_strlen=6):\n",
    "    pairs = []\n",
    "    pairs.append((part1,part2))\n",
    "    return pairs\n",
    "    \n",
    "def get_ce_pairs(sent, rec_id, use_chunks=False, chunking_method=\"nltk\", min_chunk_strlen=6):\n",
    "    cees = []\n",
    "    sent = re.sub('[\\\\s+]', ' ', sent)\n",
    "    #if len(sent) > 200:\n",
    "    #    return None\n",
    "    parts = causal_re.split(sent)\n",
    "    for i in range(0, len(parts) - 1):\n",
    "        for (cause, effect) in get_ce(parts[i],parts[i+1],use_chunks, chunking_method, min_chunk_strlen):\n",
    "            cees.append((cause, effect,rec_id, sent))\n",
    "    parts = reverse_causal_re.split(sent)\n",
    "    for i in range(0, len(parts) - 1):\n",
    "        for (cause, effect) in get_ce(parts[i+1],parts[i],use_chunks, chunking_method, min_chunk_strlen):\n",
    "            cees.append((cause, effect,rec_id, sent))\n",
    "    return cees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "lVEe058ROcpK",
    "outputId": "23ddf170-89f5-4421-ed34-d4c03291b400"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsid = 8\\nprint(len(sentences))\\nsent = sentences[sid][10]\\nprint(sent)\\nget_ce_pairs(sent, sid)\\nsent_pos = nltk.pos_tag(nltk.word_tokenize(sent))\\nsent_pos\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sid = 8\n",
    "print(len(sentences))\n",
    "sent = sentences[sid][10]\n",
    "print(sent)\n",
    "get_ce_pairs(sent, sid)\n",
    "sent_pos = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "sent_pos\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j8CLpf65OcpO"
   },
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "          NP: {<NN|NNS>+<><NN|NNS>+}              # Verb and its suroundings\n",
    "          \"\"\"\n",
    "\n",
    "grammar = r\"\"\"\n",
    "  NP: {<PDT|DT|JJ|NN.*|\\(|\\)>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN|DT><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+(<\\.>)?$} # Chunk verbs and their arguments\n",
    "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "  \"\"\"\n",
    "cp = nltk.RegexpParser(grammar, loop=1)\n",
    "\n",
    "def get_verb_left_right(sent):\n",
    "    verb = None\n",
    "    left_np = None\n",
    "    right_np = None\n",
    "    sent_pos = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "    parsed_sent = cp.parse(sent_pos)\n",
    "    for node in parsed_sent:\n",
    "        if node.__class__.__name__=='tuple':\n",
    "            continue\n",
    "        if node.label()=='NP':\n",
    "            left_np = node\n",
    "        if node.label()=='VP':  \n",
    "            vp = node\n",
    "            verb = vp[0][0]\n",
    "            if len(vp)>1:\n",
    "                if vp[1].__class__.__name__!='tuple' and vp[1].label()=='PP':\n",
    "                    verb += \" \" + vp[1][0][0]\n",
    "            for vnode in vp[1]:\n",
    "                if vnode.__class__.__name__=='tuple':\n",
    "                    continue\n",
    "                if vnode.label()=='NP':\n",
    "                    right_np = vnode\n",
    "    if verb and right_np and left_np:\n",
    "        return (verb,' '.join(v[0] for v in left_np.leaves()),' '.join(v[0] for v in right_np.leaves()))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "6-74NOf5OcpQ",
    "outputId": "c16cc306-f731-4622-eb42-ba562f240aa4"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"tuple\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-45fd68736732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_verb_left_right\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mverb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-c40b5680bdba>\u001b[0m in \u001b[0;36mget_verb_left_right\u001b[0;34m(sent)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mvp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m'tuple'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'PP'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0mverb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mvnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mvnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'tuple'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"tuple\") to str"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "Sentence_output_hilal_2 = \"Sentence_outputhilal_dummy.csv\"\n",
    "csvOutput = open(Sentence_output_hilal_2,\"w\") #<===open and write to a csv file  and New rows will be appended.\n",
    "csvWriter = csv.writer(csvOutput, quoting = csv.QUOTE_NONNUMERIC)\n",
    "csvWriter.writerow([\"file name\",\"direction\",\"verb\",\"cause\",\"effect\",\"sent\"])\n",
    "\n",
    "\n",
    "count = 0\n",
    "#breakpoint()\n",
    "for rid in sentences:\n",
    "    for sent in sentences[rid]:\n",
    "        parts = get_verb_left_right(sent)\n",
    "        if parts:\n",
    "            verb = parts[0]\n",
    "            if verb.lower() in causal_verbs:\n",
    "                count += 1\n",
    "                cause = parts[1]\n",
    "                effect = parts[2]\n",
    "                print(\"Found causal verb and cause-effect pairs: ({},{},{}) from sentence: {}\".format(verb,cause,effect,sent))\n",
    "                csvWriter.writerow([filenames[rid],\"forward\",verb,cause,effect,sent])\n",
    "            if verb.lower() in reverse_causal_verbs:\n",
    "                count += 1\n",
    "                cause = parts[2]\n",
    "                effect = parts[1]\n",
    "                csvWriter.writerow([filenames[rid],\"reverse\",verb,cause,effect,sent])\n",
    "\n",
    "                print(\"Found reverse causal verb and cause-effect pair: ({},{},{}) from sentence: {}\".format(verb,cause,effect,sent))        \n",
    "                print(\"Found {} cause-effect pairs.\".format(count))\n",
    "                \n",
    "\n",
    "print(\"end of loop\")\n",
    "\n",
    "csvOutput.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzKWzwmgOcpS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OABuZBnrOcpU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sentence_looped.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
